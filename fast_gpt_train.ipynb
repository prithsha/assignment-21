{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from model_training import GPTConfig, train, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE UPDATE HERE\n",
    "max_lr = 6e-4 \n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 50\n",
    "max_steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_config = GPTConfig()\n",
    "gpt_config.B = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training\")\n",
    "train(warmup_steps=warmup_steps,\n",
    "      max_steps=max_steps,\n",
    "      min_lr=min_lr,\n",
    "      max_lr=max_lr, \n",
    "      gpt_config=gpt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "enc = tiktoken.get_encoding('gpt2') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model = GPT(gpt_config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "out_dir = 'out'\n",
    "model.load_state_dict(torch.load(os.path.join(out_dir, 'ckpt.pt')))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"what services he has done for his country ?\"\n",
    "tokens = enc.encode(text)\n",
    "tokens = torch.tensor(tokens)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = tokens.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def generate_output(model, input_tokens, num_return_sequences, max_length):\n",
    "    x = input_tokens\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    while x.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0] # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "    output = []\n",
    "    # print the generated text\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = x[i, :max_length].tolist()\n",
    "        output.append(tokens)\n",
    "        # decoded = enc.decode(tokens)\n",
    "\n",
    "        # print(\">\", decoded)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_output(model=model, input_tokens=x,num_return_sequences=5,max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">what services he has done for his country ? we\n",
      " the, and of you a your it and what.And will you,:\n",
      " the.\n",
      ">what services he has done for his country ?!To\n",
      "\n",
      "\n",
      "\n",
      " and what, and- that:\n",
      "What you: I with: of\n",
      ">what services he has done for his country ?\n",
      "\n",
      ";\n",
      " with me but!\n",
      " but will, to the you, to my:\n",
      "\n",
      "\n",
      ">what services he has done for his country ?; a\n",
      "\n",
      "\n",
      "\n",
      "I's.O,\n",
      "\n",
      " but;, all to me:\n",
      "\n",
      ">what services he has done for his country ? you's,;\n",
      "\n",
      "The'd is my-,?\n",
      "My I; I.\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    decoded = enc.decode(result)\n",
    "    print(f\">{decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
